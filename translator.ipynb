{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import spacy\n",
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "#from datasets import load_datasets\n",
    "from typing import Iterable, List\n",
    "from torch.utils.data import DataLoader\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunk of code below loads in the data as well as a data tokenizer to split the data into tokens or language elements like words punctuation and grammar. We also need to define other special tokens such as BOS, EOS, UNK and PAD which will themselves are very important to ensure the model can fit into a tensor as well as follows the necessary requirements of the architecture (such as right shift). The last part of the code builds the vocab object, a bit like a dictionary but it also stores the special tokens and punctuation of the language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TAKEN FROM PYTORCH WEBSITE \n",
    "\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TGT_LANGUAGE = 'de'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "#The tokenizer just breaks sentences up into tokens i.e. words, punctuation etc. This code is downloading the tokenizer from spacy\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer(\"spacy\", language=\"de_core_news_sm\")\n",
    "\n",
    "#Converts a sentence into its tokens \n",
    "def yield_tokens(data_iter, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        tokenizer = token_transform[language]\n",
    "        yield tokenizer(data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and their indices in the vocab list \n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "\n",
    "#BULDING THE VOCAB\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    #Creates the vocab object inside vocab_transform and places the special tokens first in their index order\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to be able to use the data, taking it from a batch of sentences and converting it into tensors. This includes adding the special tokens where necessary (primarily this is the EOS and BOS tokens), we then need to PAD so that all sentence lengths are the same length as the longest length of the batch. Lastly we need to convert these sentences into tensors bu first tokenising and then embedding. Once that is all complete we are ready to start running the data through the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence \n",
    "\n",
    "# Joins multiple transformations on the dataset\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "#function to add BOS/EOS and create tensor. The inputs are the index inside the vocab object \n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]: #\"en\" of \"de\" in our case \n",
    "    #creates dictionary entries with code \"en\" or \"de\" with value equal to \n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization (vocab_transform[SRC_language][\"on\"] will return the index in the vocab object\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors which are ready for the model, \n",
    "# this function is passed directly into the dataloader \n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part of dealing with the data is that we require a masking function so that the model isnt looking ahead so to speak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)) #-inf on upper diagonal and 0 elsewhere\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    #Mask tensor \n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    #Tensor of False because we dont mask the source sequence for our purpose however if you use the transformer package, \n",
    "    # from pytorch it expects a masking argument. \n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    #Coloumn vector of Trues where padding occurs and False otherwise since we dont want to attend to these positions,\n",
    "    #they dont tell us any new information. \n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be worth also testing en_core_web_md the md meaning \"medium\" instead of small which uitilises word vectors unlike the current download. \n",
    "this can be done using \n",
    "python3.11 -m spacy download en_core_web_md\n",
    "python3.11 -m spacy download de_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m A \u001b[38;5;241m=\u001b[39m PositionalEncoding()\n\u001b[1;32m     43\u001b[0m b \u001b[38;5;241m=\u001b[39m A((\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m200\u001b[39m))\n\u001b[0;32m---> 44\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)        \n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "#COMPLETED ENTIRELY INDEPENDENTLY \n",
    "\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#The input embedding is then the vectorised text sequence. \n",
    "#The embedding module creates and stores the word embeddings in a useful way that are accesible via indices\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * np.sqrt(self.emb_size)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Compute Sinusoidal Positional Encoding\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, dim):\n",
    "        #POS is the position in the sentence \n",
    "        #d_model is the dimension in the encoded space \n",
    "        PE = torch.zeros((dim[0], dim[-1]))\n",
    "        d_model = dim[-1]\n",
    "        pos = dim[0]\n",
    "        PE += torch.arange(pos).unsqueeze(1)\n",
    "        i_vec_sin = torch.arange(0, d_model, 2)/d_model\n",
    "        i_vec_cos = torch.arange(1, d_model, 2)/d_model\n",
    "        PE[:, ::2] = torch.sin(torch.exp(torch.log(PE[:, ::2]) - i_vec_sin * np.log(10000)))\n",
    "        PE[:, 1::2] = torch.cos(torch.exp(torch.log(PE[:, 1::2]) - i_vec_cos * np.log(10000)))\n",
    "        return PE.float()\n",
    "    \n",
    "A = PositionalEncoding()\n",
    "b = A((200, 200))\n",
    "plt.imshow(b[0, :, :])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single layer of attention \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int):\n",
    "        \"\"\"\n",
    "        d_model -> Embedding space\n",
    "        d_k -> d_model/h where h is the number of heads of the MultiHead Attention \n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.wq = nn.Linear(d_model, d_k, device=DEVICE)\n",
    "        self.wk = nn.Linear(d_model, d_k, device=DEVICE)\n",
    "        self.wv = nn.Linear(d_model, d_k, device=DEVICE)\n",
    "        self.dmodel = d_model\n",
    "        self.dk = d_k\n",
    "        self.smax = nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, x, x_padding_mask):\n",
    "        \"\"\"\n",
    "        \n",
    "        x -> input tensor (generally word sequence)\n",
    "        x_padding_mask -> padding mask for src\n",
    "        \"\"\"\n",
    "        #Self-Attention\n",
    "        q = x @ self.wq\n",
    "        k = x @ self.wk\n",
    "        v = x @ self.wv\n",
    "\n",
    "        score = q @ k.transpose(2,3) / np.sqrt(self.dk)\n",
    "        out = self.smax(score + x_padding_mask) @ v\n",
    "        return out, score\n",
    "    \n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int):\n",
    "        \"\"\"\n",
    "        d_model -> Embedding space\n",
    "        d_k -> d_model/h where h is the number of heads of the MultiHead Attention \n",
    "        \"\"\"\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.wq = nn.Linear(d_model, d_k, device=DEVICE)\n",
    "        self.wk = nn.Linear(d_model, d_k, device=DEVICE)\n",
    "        self.wv = nn.Linear(d_model, d_k, device=DEVICE)\n",
    "        self.dmodel = d_model\n",
    "        self.dk = d_k\n",
    "        self.smax = nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, x, x_padding_mask, y):\n",
    "        \"\"\"\n",
    "        \n",
    "        x -> input tensor (generally word sequence)\n",
    "        x_padding_mask -> padding mask for src\n",
    "        \"\"\"\n",
    "        #Self-Attention\n",
    "        q = y @ self.wq\n",
    "        k = x @ self.wk\n",
    "        v = x @ self.wv\n",
    "\n",
    "        score = q @ k.transpose(2,3) / np.sqrt(self.dk)\n",
    "        out = self.smax(score + x_padding_mask) @ v\n",
    "        return out, score\n",
    "\n",
    "#Single layer of attention \n",
    "class MaskedAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_k: int):\n",
    "        \"\"\"\n",
    "        d_model -> Embedding space\n",
    "        d_k -> d_model/h where h is the number of heads of the MultiHead Attention \n",
    "        \"\"\"\n",
    "        super(MaskedAttention, self).__init__()\n",
    "        self.wq = nn.Linear(d_model, d_k, device=DEVICE)\n",
    "        self.wk = nn.Linear(d_model, d_k, device=DEVICE)\n",
    "        self.wv = nn.Linear(d_model, d_k, device=DEVICE)\n",
    "        self.dmodel = d_model\n",
    "        self.dk = d_k\n",
    "        self.smax = nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, y, y_mask, y_padding_mask):\n",
    "        \"\"\"\n",
    "        \n",
    "        x -> input tensor (generally word sequence)\n",
    "        x_padding_mask -> padding mask for src\n",
    "        other arguments -> tgt, tgt mask, tgt padding mask \n",
    "        \"\"\"\n",
    "\n",
    "        #In a self attention block x=y, in the encoder-decoder attention only the keys and values come from the encoder output\n",
    "        #Encoder-Decoder Attention - needs to apply masking of output\n",
    "        q = y @ self.wq\n",
    "        k = y @ self.wk\n",
    "        v = y @ self.wv\n",
    "\n",
    "        score = q @ k.transpose(2,3) / np.sqrt(self.dk)\n",
    "        out = self.smax(score + y_mask + y_padding_mask) @ v\n",
    "        return out, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-Head Attention \n",
    "#h is the number of heads that we will use (same letter as used in the paper)\n",
    "\n",
    "class MultiHeadSA(nn.Module):\n",
    "    def __init__(self, h: int, d_model: int):\n",
    "        \"\"\"\n",
    "        h -> number of heads \n",
    "        d_model -> dimension of the model embedding\n",
    "        \"\"\"\n",
    "        super(MultiHeadSA, self).__init__()\n",
    "        self.dk = int(d_model / h)\n",
    "        self.dmodel = d_model \n",
    "        self.h = h\n",
    "        self.weights = nn.ModuleList([SelfAttention(self.dmodel, self.dk) for _ in range(self.h)])\n",
    "        self.W = nn.Linear(self.dmodel, self.dmodel)\n",
    "\n",
    "    def forward(self, x, x_padding_mask, *args):\n",
    "        outputs = []\n",
    "        for attention_layer in self.weights:\n",
    "            out, score = attention_layer(x, x_padding_mask, *args)\n",
    "            outputs.append(out)\n",
    "        concat_output = outputs[0]\n",
    "        for i in outputs[1:]:\n",
    "            concat_output = torch.cat((concat_output, i), 0)\n",
    "        return concat_output\n",
    "    \n",
    "#Multi-Head Attention \n",
    "#h is the number of heads that we will use (same letter as used in the paper)\n",
    "\n",
    "class MultiHeadCA(nn.Module):\n",
    "    def __init__(self, h: int, d_model: int):\n",
    "        \"\"\"\n",
    "        h -> number of heads \n",
    "        d_model -> dimension of the model embedding\n",
    "        \"\"\"\n",
    "        super(MultiHeadCA, self).__init__()\n",
    "        self.dk = int(d_model / h)\n",
    "        self.dmodel = d_model \n",
    "        self.h = h\n",
    "        self.weights = nn.ModuleList([CrossAttention(self.dmodel, self.dk) for _ in range(self.h)])\n",
    "        self.W = nn.Linear(self.dmodel, self.dmodel)\n",
    "\n",
    "    def forward(self, x, x_padding_mask, y):\n",
    "        outputs = []\n",
    "        for attention_layer in self.weights:\n",
    "            out, score = attention_layer(x, x_padding_mask, y)\n",
    "            outputs.append(out)\n",
    "        concat_output = outputs[0]\n",
    "        for i in outputs[1:]:\n",
    "            concat_output = torch.cat((concat_output, i), 0)\n",
    "        return concat_output\n",
    "    \n",
    "\n",
    "#Multi-Head Attention \n",
    "#h is the number of heads that we will use (same letter as used in the paper)\n",
    "\n",
    "class MultiHeadMA(nn.Module):\n",
    "    def __init__(self, h: int, d_model: int):\n",
    "        \"\"\"\n",
    "        h -> number of heads \n",
    "        d_model -> dimension of the model embedding\n",
    "        \"\"\"\n",
    "        super(MultiHeadMA, self).__init__()\n",
    "        self.dk = int(d_model / h)\n",
    "        self.dmodel = d_model \n",
    "        self.h = h\n",
    "        self.weights = nn.ModuleList([MaskedAttention(self.dmodel, self.dk) for _ in range(self.h)])\n",
    "        self.W = nn.Linear(self.dmodel, self.dmodel)\n",
    "\n",
    "    def forward(self, y, y_mask, y_padding_mask):\n",
    "        outputs = []\n",
    "        for attention_layer in self.weights:\n",
    "            out, score = attention_layer(y, y_mask, y_padding_mask)\n",
    "            outputs.append(out)\n",
    "        concat_output = outputs[0]\n",
    "        for i in outputs[1:]:\n",
    "            concat_output = torch.cat((concat_output, i), 0)\n",
    "        return concat_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FeedForward Neural Network\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_hidden: int):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.layer1 = nn.Linear(d_model, d_hidden, device=DEVICE)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(d_hidden, d_model, device=DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        d_model -> Dimension of Model \n",
    "        eps -> Value to prevent Division by zero \n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x -> Normalisation input \n",
    "        \"\"\"\n",
    "\n",
    "        mu = x.mean(dim=-1)\n",
    "        std = x.std(dim=-1)\n",
    "\n",
    "        x_norm = (x - mu)/(std + self.eps)\n",
    "        output = x_norm * self.gamma + self.beta \n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, h: int, d_model: int, d_hidden):\n",
    "        \"\"\"\n",
    "        h -> Number of Attention heads\n",
    "        d_model -> Dimension of the embedding space \n",
    "        d_hidden -> Number of hidden layers in the feedforward network\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.mha = MultiHeadSA(h, d_model)\n",
    "        self.ffn = FeedForward(d_model, d_hidden)\n",
    "        self.layernorm1 = LayerNorm(d_model)\n",
    "        self.layernorm2 = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, x_padding_mask):\n",
    "        \"\"\"\n",
    "        x -> Input \n",
    "        \"\"\"\n",
    "\n",
    "        x += self.mha(x, x_padding_mask)\n",
    "        x = self.layernorm1(x)\n",
    "        x += self.ffn(x)\n",
    "        x = self.layernorm2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, h, d_model, d_hidden):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.masked = MultiHeadMA(h, d_model)\n",
    "        #eda - encoder- decoder attention\n",
    "        self.ca = MultiHeadCA(h, d_model)\n",
    "        self.ffn = FeedForward(d_model, d_hidden)\n",
    "        self.layernorm1 = LayerNorm(d_model)\n",
    "        self.layernorm2 = LayerNorm(d_model)\n",
    "        self.layernorm3 = LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, x_padding_mask, y, y_mask, y_padding_mask):\n",
    "        \"\"\"\n",
    "        x -> Output from encoder \n",
    "        y -> target sequence \n",
    "        \"\"\"\n",
    "        y += self.masked(y, y_mask, y_padding_mask)\n",
    "        y = self.layernorm1(y)\n",
    "        y += self.ca(x, x_padding_mask, y)\n",
    "        y = self.layernorm2(y)\n",
    "        y += self.ffn(y)\n",
    "        y = self.layernorm3(y)\n",
    "        return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, h, N, d_model, d_hidden, src_vocab_size, tgt_vocab_size):\n",
    "        \"\"\"\n",
    "        x -> Input sequence\n",
    "        y -> Output sequence \n",
    "        h -> Number of heads \n",
    "        N -> Number of Encoder and Decoder layers \n",
    "        d_model -> Dimension of the model \n",
    "        d_hidden -> Number of hidden layers for the feed-forward model\n",
    "        d_input -> Dimension of input (size of input language)\n",
    "\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        self.EncoderLayers = nn.ModuleList()\n",
    "        self.DecoderLayers = nn.ModuleList()\n",
    "        for _ in range(N):\n",
    "            self.EncoderLayers.append(Encoder(h, d_model, d_hidden))\n",
    "            self.DecoderLayers.append(Decoder(h, d_model, d_hidden))\n",
    "        self.Linearlayer = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.smax = nn.Softmax(tgt_vocab_size)\n",
    "        self.pe = PositionalEncoding()\n",
    "        self.src_embed = TokenEmbedding(src_vocab_size, d_model)\n",
    "        self.tgt_embed = TokenEmbedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "    def transform_src(self, x):\n",
    "        return self.src_embed(x)\n",
    "    \n",
    "    def forward(self, x, x_padding_mask, y, y_mask, y_padding_mask):\n",
    "        x = x.float()\n",
    "        y = y.float()\n",
    "        x += self.pe(x.size())\n",
    "        y += self.pe(y.size())\n",
    "\n",
    "        for encoder in self.EncoderLayers:\n",
    "            x = encoder(x, x_padding_mask)\n",
    "        for decoder in self.DecoderLayers:\n",
    "            y = decoder(x, x_padding_mask, y, y_mask, y_padding_mask)\n",
    "        y = self.Linearlayer(y)\n",
    "        return self.smax(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the train function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "src_vocab_size = len(vocab_transform[SRC_LANGUAGE])\n",
    "tgt_vocab_size = len(vocab_transform[TGT_LANGUAGE])\n",
    "d_hidden = 128\n",
    "#Number of attention heads\n",
    "h = 8\n",
    "#Number of encoder and decoder layers - (this can be coded so that the number of encoder and decoder layers are different)\n",
    "N = 4\n",
    "d_model = 512\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "transformer = Transformer(h, N, d_model, d_hidden, src_vocab_size, tgt_vocab_size)\n",
    "\n",
    "#Initialise parameters \n",
    "for param in transformer.parameters():\n",
    "    if param.dim() > 1:\n",
    "        nn.init.xavier_uniform_(param)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimiser = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "def train(model, optimiser):\n",
    "    #inform the model that we are in training mode - this ensures dropout and batchnorm occur as expected \n",
    "    model.train()\n",
    "    #initalise loss\n",
    "    losses = 0\n",
    "    #Data Loader\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        #We dont need an EOS token for the tgt hence up till -1\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        output = model(src, src_padding_mask, tgt_input, tgt_mask, tgt_padding_mask)\n",
    "        optimiser.zero_grad()\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses/len(list(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 128])\n",
      "torch.Size([24, 128])\n",
      "torch.FloatTensor\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for @: 'Tensor' and 'Linear'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[101], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimiser)\u001b[0m\n\u001b[1;32m     38\u001b[0m tgt_input \u001b[38;5;241m=\u001b[39m tgt[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     39\u001b[0m src_mask, tgt_mask, src_padding_mask, tgt_padding_mask \u001b[38;5;241m=\u001b[39m create_mask(src, tgt_input)\n\u001b[0;32m---> 40\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     42\u001b[0m tgt_out \u001b[38;5;241m=\u001b[39m tgt[\u001b[38;5;241m1\u001b[39m:, :]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[100], line 39\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, x_padding_mask, y, y_mask, y_padding_mask)\u001b[0m\n\u001b[1;32m     36\u001b[0m y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe(y\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m encoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEncoderLayers:\n\u001b[0;32m---> 39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDecoderLayers:\n\u001b[1;32m     41\u001b[0m     y \u001b[38;5;241m=\u001b[39m decoder(x, x_padding_mask, y, y_mask, y_padding_mask)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[77], line 19\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, x_padding_mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, x_padding_mask):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    x -> Input \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm1(x)\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(x)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[74], line 20\u001b[0m, in \u001b[0;36mMultiHeadSA.forward\u001b[0;34m(self, x, x_padding_mask, *args)\u001b[0m\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attention_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights:\n\u001b[0;32m---> 20\u001b[0m     out, score \u001b[38;5;241m=\u001b[39m \u001b[43mattention_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m     22\u001b[0m concat_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[73], line 23\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, x, x_padding_mask)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mx -> input tensor (generally word sequence)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03mx_padding_mask -> padding mask for src\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#Self-Attention\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwq\u001b[49m\n\u001b[1;32m     24\u001b[0m k \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwk\n\u001b[1;32m     25\u001b[0m v \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for @: 'Tensor' and 'Linear'"
     ]
    }
   ],
   "source": [
    "train(transformer, optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
